{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download livedoor news corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "livedoor_news  livedoor_sentences_nva.pkl\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ./data/corpus/livedoor\n",
    "!mkdir -p ./data/model\n",
    "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
    "!tar -zxf ldcc-20140209.tar.gz -C ./data/corpus/livedoor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir = './data/corpus/livedoor/'\n",
    "model_dir = './data/moel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP\n",
    "import re\n",
    "import MeCab\n",
    "mecab = MeCab.Tagger (r\"-Ochasen\") #  -u ./dict/qiita.dic\n",
    "mecab.parse(\"\")\n",
    "\n",
    "# SCDV\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# ML\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# japanese plot setting\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "font_path = \"/usr/share/fonts/truetype/takao-gothic/TakaoGothic.ttf\"\n",
    "font_prop = FontProperties(fname=font_path)\n",
    "mpl.rcParams['font.family'] = font_prop.get_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloudpickle\n",
    "\n",
    "import cloudpickle\n",
    "\n",
    "def load_from_pkl( fpath ):\n",
    "    frb = open(fpath, 'rb')\n",
    "    obj = cloudpickle.loads(frb.read())\n",
    "    return obj\n",
    "\n",
    "def save_as_pkl( obj, fpath ):\n",
    "    fwb = open( fpath, 'wb')\n",
    "    tmp = fwb.write(cloudpickle.dumps(obj))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "\n",
    "def get_tokens( text, ):\n",
    "    tokens = []\n",
    "    result = mecab.parseToNode( text )\n",
    "    while result:\n",
    "        tokens.append( result.surface )\n",
    "        result = result.next\n",
    "    return tokens\n",
    "\n",
    "def get_tokens_pos( text, target_pos ):\n",
    "    tokens = []\n",
    "    result = mecab.parseToNode( text )\n",
    "    while result:\n",
    "        pos = result.feature.split(',')[0]\n",
    "        if pos in target_pos: tokens.append( result.surface )\n",
    "        result = result.next\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCDV(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    w2v_model: \n",
    "    softclustering_model: \n",
    "    sparse_percentage: the threshold percentage for making it sparse\n",
    "    \"\"\"\n",
    "    def __init__(self, w2v_model, sc_model, sparse_percentage):\n",
    "        \n",
    "        # values\n",
    "        self.w2v_model = w2v_model\n",
    "        self.num_clusters = sc_model.n_components\n",
    "        self.w2v_vector_size = w2v_model.vector_size\n",
    "        self.min_no = .0\n",
    "        self.max_no = .0\n",
    "        self.sparse_percentage = sparse_percentage\n",
    "        \n",
    "        # apply soft clustering to embedding vectors\n",
    "        self.w2v_vectors = w2v_model.wv.vectors\n",
    "        idx, idx_proba = self._soft_clustering( sc_model, self.w2v_vectors)\n",
    "        self.word_centroid_map = dict( zip( w2v_model.wv.index2word, idx ) ) # Create a Word / Index dictionary, mapping each vocabulary word to a cluster number\n",
    "        self.word_centroid_prob_map = dict(zip( w2v_model.wv.index2word, idx_proba )) # Create a Word / Probability of cluster assignment dictionary, mapping each vocabulary word to list of probabilities of cluster assignments.       \n",
    "               \n",
    "        return\n",
    "    \n",
    "    \"\"\"\n",
    "    sentences:\n",
    "    \"\"\"\n",
    "    def precompute_word_topic_vector( self, sentences ):\n",
    "               \n",
    "        # compute idf values\n",
    "        self.featurenames, self.word_idf_dict = self._compute_idf_values( sentences ) \n",
    "        \n",
    "        # compute word topic vectors\n",
    "        self.wv = self._get_probability_word_vectors( self.w2v_model,\n",
    "                                                      self.num_clusters,\n",
    "                                                      self.w2v_vector_size,\n",
    "                                                      self.featurenames,\n",
    "                                                      self.word_centroid_map,\n",
    "                                                      self.word_centroid_prob_map,\n",
    "                                                      self.word_idf_dict)\n",
    "        return\n",
    "    \n",
    "    def train( self, sentences ):\n",
    "\n",
    "        # values\n",
    "        doc_num = len(sentences)\n",
    "        \n",
    "        # get document vector\n",
    "        X = np.zeros( (doc_num, self.num_clusters*self.w2v_vector_size), dtype=\"float32\")\n",
    "        for idx, tokens in enumerate( sentences ):\n",
    "            X[idx] = self._create_cluster_vector_and_gwbowv(self.wv, tokens, self.word_centroid_map, self.word_centroid_prob_map, self.w2v_vector_size, self.word_idf_dict, self.featurenames, self.num_clusters, train=True)\n",
    "\n",
    "        # get the threshold value for making it sparse. \n",
    "        thres = (abs( self.max_no / float( doc_num ) ) + abs( self.min_no / float( doc_num ) )) / 2\n",
    "        self.sparse_thres = thres * self.sparse_percentage\n",
    "        \n",
    "        # Make values of matrices which are less than threshold to zero.\n",
    "        temp = abs(X) < self.sparse_thres\n",
    "        X[temp] = 0\n",
    "\n",
    "        return X\n",
    "\n",
    "    def infer_vector( self, sentences ):\n",
    "        \n",
    "        # values\n",
    "        doc_num = len(sentences)\n",
    "        \n",
    "        # get document vector\n",
    "        X = np.zeros( (doc_num, self.num_clusters*self.w2v_vector_size), dtype=\"float32\")\n",
    "        for idx, tokens in enumerate( sentences ):\n",
    "            X[idx] = self._create_cluster_vector_and_gwbowv(self.wv, tokens, self.word_centroid_map, self.word_centroid_prob_map, self.w2v_vector_size, self.word_idf_dict, self.featurenames, self.num_clusters, train=False)\n",
    "       \n",
    "        # Make values of matrices which are less than threshold to zero.\n",
    "        temp = abs(X) < self.sparse_thres\n",
    "        X[temp] = 0        \n",
    "\n",
    "        return X\n",
    "\n",
    "    \n",
    "    def _create_cluster_vector_and_gwbowv( self, prob_wordvecs, wordlist, word_centroid_map, word_centroid_prob_map, dimension, word_idf_dict, featurenames, num_centroids, train=False):\n",
    "        \n",
    "        bag_of_centroids = np.zeros( num_centroids * dimension, dtype=\"float32\" )\n",
    "        for word in wordlist:\n",
    "            if not word in word_centroid_map: continue  \n",
    "            bag_of_centroids += prob_wordvecs[word]\n",
    "\n",
    "        norm = np.sqrt(np.einsum('...i,...i', bag_of_centroids, bag_of_centroids))\n",
    "        if norm != 0: bag_of_centroids /= norm\n",
    "\n",
    "        # to make feature vector sparse, make note of minimum and maximum values.\n",
    "        if train:\n",
    "            self.min_no += min(bag_of_centroids)\n",
    "            self.max_no += max(bag_of_centroids)\n",
    "\n",
    "        return bag_of_centroids\n",
    "    \n",
    "    @staticmethod\n",
    "    def _soft_clustering( sc_model, word_vectors):\n",
    "        \n",
    "        sc_model.fit(word_vectors)\n",
    "        idx = sc_model.predict(word_vectors)\n",
    "        idx_proba = sc_model.predict_proba(word_vectors)\n",
    "\n",
    "        return (idx, idx_proba)\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_idf_values( sentences ):\n",
    "        \n",
    "        tfv = TfidfVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x, dtype=np.float32)\n",
    "        tfidfmatrix_traindata = tfv.fit_transform(sentences)\n",
    "        featurenames = tfv.get_feature_names()\n",
    "        idf = tfv._tfidf.idf_\n",
    "        idf_dict = { pair[0]:pair[1] for pair in zip(featurenames, idf) }\n",
    "        \n",
    "        return featurenames, idf_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_probability_word_vectors( w2v_model, num_clusters, num_features, featurenames, word_centroid_map, word_centroid_prob_map, word_idf_dict):\n",
    "        \n",
    "        prob_wordvecs = {}\n",
    "        for word in word_centroid_map:\n",
    "            prob_wordvecs[word] = np.zeros( num_clusters * num_features, dtype=\"float32\" )\n",
    "            for c_idx in range(0, num_clusters):\n",
    "                if not word in word_idf_dict: continue\n",
    "                prob_wordvecs[word][c_idx*num_features:(c_idx+1)*num_features] = w2v_model.wv[word] * word_centroid_prob_map[word][c_idx] * word_idf_dict[word]\n",
    "        \n",
    "        return prob_wordvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dir = '%s/text' % corpus_dir\n",
    "files = os.listdir(text_dir)\n",
    "class_list = [f for f in files if os.path.isdir(os.path.join(text_dir, f)) ]\n",
    "\n",
    "skipline = 2\n",
    "y = []\n",
    "texts = []\n",
    "for c in class_list:\n",
    "    c_dir = '%s/%s' % (text_dir, c)\n",
    "    for file in os.listdir( c_dir ):\n",
    "        if file == 'LICENSE.txt': continue\n",
    "        with open( '%s/%s' % ( c_dir, file ) ) as f:\n",
    "            for _ in range( skipline ): next(f)\n",
    "            text = f.read()\n",
    "            texts.append( text )\n",
    "            y.append( c )\n",
    "y = np.array(y)\n",
    "doc_num = len( texts )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input value\n",
    "- tokenization\n",
    " - 名詞のみ, 内容語(名詞, 形容詞, 動詞), all\n",
    "- w2v model\n",
    " - model\n",
    "   - word2vec, fastText\n",
    " - feature_dim\n",
    "   - 50, 100, 200\n",
    "- soft clustering (GMM)\n",
    "   - cluster_num\n",
    "     - 10, 20, 30, 40, 50\n",
    "- sparse percentage\n",
    " - 0.1, 0.2, ..., 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_list = [('n',['名詞']), ('nva',['名詞', '動詞', '形容詞']), ('all','all')]\n",
    "word_embedding_list = [ 'word2vec', 'fastText' ]\n",
    "num_features_list = [50, 100, 200] # fastText embedding dim\n",
    "num_clusters_list = [10, 20, 30, 40, 50]\n",
    "sparse_percentage_list = [ i/10 for i in range( 1, 6) ]\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # parameters\n",
    "    token_type = trial.suggest_categorical('token_type', tokenize_list)\n",
    "    word_embedding = trial.suggest_categorical('word_embedding', word_embedding_list)\n",
    "    num_features = trial.suggest_categorical('num_features', num_features_list)\n",
    "    num_clusters = trial.suggest_categorical('num_clusters', num_clusters_list)\n",
    "    sparse_percentage = trial.suggest_categorical('sparse_percentage', sparse_percentage_list)\n",
    "    \n",
    "    # load values\n",
    "    sentences = load_from_pkl( '/data/corpus/livedoor_sentences_%s.pkl' % token_type[0] )\n",
    "    \n",
    "    if word_embedding == 'word2vec':\n",
    "        w2v_model = Word2Vec.load( '/data/model/livedoor_%s_%s_%s.model' % ( word_embedding, num_features, token_type[0] ) )\n",
    "    if word_embedding == 'fastText':\n",
    "        w2v_model = FastText.load( '/data/model/livedoor_%s_%s_%s.model' % ( word_embedding, num_features, token_type[0] ) )\n",
    "    \n",
    "    # set gmm\n",
    "    gmm =  GaussianMixture(n_components=num_clusters, covariance_type=\"tied\", init_params='kmeans', max_iter=50)\n",
    "       \n",
    "    # learn (5-fold cv)\n",
    "    acc_score_cv = []\n",
    "    kf = StratifiedKFold(n_splits=5, random_state=777, shuffle=True)\n",
    "    for idx, (train, val) in enumerate( kf.split(sentences, y) ):\n",
    "\n",
    "        sentences_train = [ sentences[i] for i in train ]\n",
    "        sentences_val = [ sentences[i] for i in val ]\n",
    "\n",
    "        # train scdv\n",
    "        scdv_model = SCDV(w2v_model=w2v_model, sc_model=gmm, sparse_percentage = sparse_percentage )\n",
    "        scdv_model.precompute_word_topic_vector(sentences_train)\n",
    "        X_train = scdv_model.train(sentences_train)\n",
    "\n",
    "        # learn model\n",
    "        clf = lgb.LGBMClassifier(n_estimators=10, objective=\"multiclass\")\n",
    "        clf.fit(X_train, y[train])\n",
    "\n",
    "        # test scdv\n",
    "        X_val = scdv_model.infer_vector(sentences_val)\n",
    "\n",
    "        # predict validation and score\n",
    "        preds = clf.predict(X_val)\n",
    "        acc_cv = accuracy_score( preds, y[val] )\n",
    "        acc_score_cv.append( acc_cv )\n",
    "\n",
    "    # cv score\n",
    "    mean = sum( acc_score_cv ) / len( acc_score_cv )\n",
    "    return 1.0 - mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "\n",
    "for token_type in tokenize_list:\n",
    "    \n",
    "    target_pkl = '%s/livedoor_sentences_%s.pkl' % (corpus_dir, token_type[0])\n",
    "    if os.path.exists( target_pkl ): continue\n",
    "    print( token_type )\n",
    "    \n",
    "    if token_type[0] == 'all':\n",
    "        sentences = [ get_tokens(text) for text in texts ]\n",
    "    else:\n",
    "        sentences = [ get_tokens_pos(text, token_type[1]) for text in texts ]\n",
    "    \n",
    "    # save tokenized sentences\n",
    "    save_as_pkl( sentences, target_pkl )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn fastText\n",
    "\n",
    "# static\n",
    "min_word_count = 10 # Minimum word count\n",
    "context = 10 # Context window size\n",
    "\n",
    "for token_type in tokenize_list:\n",
    "    for word_embedding in word_embedding_list:\n",
    "        for num_features in num_features_list:\n",
    "\n",
    "            target_model = '%s/livedoor_%s_%s_%s.model' % ( model_dir, word_embedding, num_features, token_type[0] )\n",
    "            if os.path.exists( target_model ): continue\n",
    "            print( word_embedding, num_features, token_type[0] )\n",
    "\n",
    "            if word_embedding == 'word2vec':\n",
    "                model = Word2Vec(size=num_features, sg=1, workers=7, window=context, min_count=min_word_count)\n",
    "            if word_embedding == 'fastText':\n",
    "                model = FastText(size=num_features, sg=1, workers=7, window=context, min_count=min_word_count)\n",
    "            \n",
    "            sentences = load_from_pkl( '%s/livedoor_sentences_%s.pkl' % (corpus_dir, token_type[0]) )\n",
    "            \n",
    "            model.build_vocab( sentences )\n",
    "            model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "            # save model\n",
    "            model.save( target_model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-03-13 06:23:32,889] Finished a trial resulted in value: 0.172383289679928. Current best value is 0.172383289679928 with parameters: {'token_type': ('nva', ['名詞', '動詞', '形容詞']), 'word_embedding': 'word2vec', 'num_features': 200, 'num_clusters': 30, 'sparse_percentage': 0.2}.\n",
      "[I 2019-03-13 06:25:52,753] Finished a trial resulted in value: 0.2608897833270084. Current best value is 0.172383289679928 with parameters: {'token_type': ('nva', ['名詞', '動詞', '形容詞']), 'word_embedding': 'word2vec', 'num_features': 200, 'num_clusters': 30, 'sparse_percentage': 0.2}.\n",
      "[I 2019-03-13 06:30:56,624] Finished a trial resulted in value: 0.13302737885803295. Current best value is 0.13302737885803295 with parameters: {'token_type': ('all', 'all'), 'word_embedding': 'word2vec', 'num_features': 100, 'num_clusters': 50, 'sparse_percentage': 0.1}.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=10)\n",
    "print('params:', study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
