{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download livedoor news corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-03-13 07:06:20--  https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
      "Resolving www.rondhuit.com (www.rondhuit.com)... 59.106.19.174\n",
      "Connecting to www.rondhuit.com (www.rondhuit.com)|59.106.19.174|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8855190 (8.4M) [application/x-gzip]\n",
      "Saving to: ‘ldcc-20140209.tar.gz.1’\n",
      "\n",
      "ldcc-20140209.tar.g 100%[===================>]   8.44M  13.4MB/s    in 0.6s    \n",
      "\n",
      "2019-03-13 07:06:21 (13.4 MB/s) - ‘ldcc-20140209.tar.gz.1’ saved [8855190/8855190]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ./data/corpus/livedoor\n",
    "!mkdir -p ./data/model\n",
    "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
    "!tar -zxf ldcc-20140209.tar.gz -C ./data/corpus/livedoor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir = './data/corpus/livedoor/'\n",
    "model_dir = './data/moel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP\n",
    "import re\n",
    "import MeCab\n",
    "mecab = MeCab.Tagger (r\"-Ochasen\") #  -u ./dict/qiita.dic\n",
    "mecab.parse(\"\")\n",
    "\n",
    "# SCDV\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# japanese plot setting\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "font_path = \"/usr/share/fonts/truetype/takao-gothic/TakaoGothic.ttf\"\n",
    "font_prop = FontProperties(fname=font_path)\n",
    "mpl.rcParams['font.family'] = font_prop.get_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloudpickle\n",
    "\n",
    "import cloudpickle\n",
    "\n",
    "def load_from_pkl( fpath ):\n",
    "    frb = open(fpath, 'rb')\n",
    "    obj = cloudpickle.loads(frb.read())\n",
    "    return obj\n",
    "\n",
    "def save_as_pkl( obj, fpath ):\n",
    "    fwb = open( fpath, 'wb')\n",
    "    tmp = fwb.write(cloudpickle.dumps(obj))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "\n",
    "def get_tokens( text, ):\n",
    "    tokens = []\n",
    "    result = mecab.parseToNode( text )\n",
    "    while result:\n",
    "        tokens.append( result.surface )\n",
    "        result = result.next\n",
    "    return tokens\n",
    "\n",
    "def get_tokens_pos( text, target_pos ):\n",
    "    tokens = []\n",
    "    result = mecab.parseToNode( text )\n",
    "    while result:\n",
    "        pos = result.feature.split(',')[0]\n",
    "        if pos in target_pos: tokens.append( result.surface )\n",
    "        result = result.next\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCDV(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    w2v_model: \n",
    "    softclustering_model: \n",
    "    sparse_percentage: the threshold percentage for making it sparse\n",
    "    \"\"\"\n",
    "    def __init__(self, w2v_model, sc_model, sparse_percentage):\n",
    "        \n",
    "        # values\n",
    "        self.w2v_model = w2v_model\n",
    "        self.num_clusters = sc_model.n_components\n",
    "        self.w2v_vector_size = w2v_model.vector_size\n",
    "        self.min_no = .0\n",
    "        self.max_no = .0\n",
    "        self.sparse_percentage = sparse_percentage\n",
    "        \n",
    "        # apply soft clustering to embedding vectors\n",
    "        self.w2v_vectors = w2v_model.wv.vectors\n",
    "        idx, idx_proba = self._soft_clustering( sc_model, self.w2v_vectors)\n",
    "        self.word_centroid_map = dict( zip( w2v_model.wv.index2word, idx ) ) # Create a Word / Index dictionary, mapping each vocabulary word to a cluster number\n",
    "        self.word_centroid_prob_map = dict(zip( w2v_model.wv.index2word, idx_proba )) # Create a Word / Probability of cluster assignment dictionary, mapping each vocabulary word to list of probabilities of cluster assignments.       \n",
    "               \n",
    "        return\n",
    "    \n",
    "    \"\"\"\n",
    "    sentences:\n",
    "    \"\"\"\n",
    "    def precompute_word_topic_vector( self, sentences ):\n",
    "               \n",
    "        # compute idf values\n",
    "        self.featurenames, self.word_idf_dict = self._compute_idf_values( sentences ) \n",
    "        \n",
    "        # compute word topic vectors\n",
    "        self.wv = self._get_probability_word_vectors( self.w2v_model,\n",
    "                                                      self.num_clusters,\n",
    "                                                      self.w2v_vector_size,\n",
    "                                                      self.featurenames,\n",
    "                                                      self.word_centroid_map,\n",
    "                                                      self.word_centroid_prob_map,\n",
    "                                                      self.word_idf_dict)\n",
    "        return\n",
    "    \n",
    "    def train( self, sentences ):\n",
    "\n",
    "        # values\n",
    "        doc_num = len(sentences)\n",
    "        \n",
    "        # get document vector\n",
    "        X = np.zeros( (doc_num, self.num_clusters*self.w2v_vector_size), dtype=\"float32\")\n",
    "        for idx, tokens in enumerate( sentences ):\n",
    "            X[idx] = self._create_cluster_vector_and_gwbowv(self.wv, tokens, self.word_centroid_map, self.word_centroid_prob_map, self.w2v_vector_size, self.word_idf_dict, self.featurenames, self.num_clusters, train=True)\n",
    "\n",
    "        # get the threshold value for making it sparse. \n",
    "        thres = (abs( self.max_no / float( doc_num ) ) + abs( self.min_no / float( doc_num ) )) / 2\n",
    "        self.sparse_thres = thres * self.sparse_percentage\n",
    "        \n",
    "        # Make values of matrices which are less than threshold to zero.\n",
    "        temp = abs(X) < self.sparse_thres\n",
    "        X[temp] = 0\n",
    "\n",
    "        return X\n",
    "\n",
    "    def infer_vector( self, sentences ):\n",
    "        \n",
    "        # values\n",
    "        doc_num = len(sentences)\n",
    "        \n",
    "        # get document vector\n",
    "        X = np.zeros( (doc_num, self.num_clusters*self.w2v_vector_size), dtype=\"float32\")\n",
    "        for idx, tokens in enumerate( sentences ):\n",
    "            X[idx] = self._create_cluster_vector_and_gwbowv(self.wv, tokens, self.word_centroid_map, self.word_centroid_prob_map, self.w2v_vector_size, self.word_idf_dict, self.featurenames, self.num_clusters, train=False)\n",
    "       \n",
    "        # Make values of matrices which are less than threshold to zero.\n",
    "        temp = abs(X) < self.sparse_thres\n",
    "        X[temp] = 0        \n",
    "\n",
    "        return X\n",
    "\n",
    "    \n",
    "    def _create_cluster_vector_and_gwbowv( self, prob_wordvecs, wordlist, word_centroid_map, word_centroid_prob_map, dimension, word_idf_dict, featurenames, num_centroids, train=False):\n",
    "        \n",
    "        bag_of_centroids = np.zeros( num_centroids * dimension, dtype=\"float32\" )\n",
    "        for word in wordlist:\n",
    "            if not word in word_centroid_map: continue  \n",
    "            bag_of_centroids += prob_wordvecs[word]\n",
    "\n",
    "        norm = np.sqrt(np.einsum('...i,...i', bag_of_centroids, bag_of_centroids))\n",
    "        if norm != 0: bag_of_centroids /= norm\n",
    "\n",
    "        # to make feature vector sparse, make note of minimum and maximum values.\n",
    "        if train:\n",
    "            self.min_no += min(bag_of_centroids)\n",
    "            self.max_no += max(bag_of_centroids)\n",
    "\n",
    "        return bag_of_centroids\n",
    "    \n",
    "    @staticmethod\n",
    "    def _soft_clustering( sc_model, word_vectors):\n",
    "        \n",
    "        sc_model.fit(word_vectors)\n",
    "        idx = sc_model.predict(word_vectors)\n",
    "        idx_proba = sc_model.predict_proba(word_vectors)\n",
    "\n",
    "        return (idx, idx_proba)\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_idf_values( sentences ):\n",
    "        \n",
    "        tfv = TfidfVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x, dtype=np.float32)\n",
    "        tfidfmatrix_traindata = tfv.fit_transform(sentences)\n",
    "        featurenames = tfv.get_feature_names()\n",
    "        idf = tfv._tfidf.idf_\n",
    "        idf_dict = { pair[0]:pair[1] for pair in zip(featurenames, idf) }\n",
    "        \n",
    "        return featurenames, idf_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_probability_word_vectors( w2v_model, num_clusters, num_features, featurenames, word_centroid_map, word_centroid_prob_map, word_idf_dict):\n",
    "        \n",
    "        prob_wordvecs = {}\n",
    "        for word in word_centroid_map:\n",
    "            prob_wordvecs[word] = np.zeros( num_clusters * num_features, dtype=\"float32\" )\n",
    "            for c_idx in range(0, num_clusters):\n",
    "                if not word in word_idf_dict: continue\n",
    "                prob_wordvecs[word][c_idx*num_features:(c_idx+1)*num_features] = w2v_model.wv[word] * word_centroid_prob_map[word][c_idx] * word_idf_dict[word]\n",
    "        \n",
    "        return prob_wordvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dir = '%s/text' % corpus_dir\n",
    "files = os.listdir(text_dir)\n",
    "class_list = [f for f in files if os.path.isdir(os.path.join(text_dir, f)) ]\n",
    "\n",
    "skipline = 2\n",
    "y = []\n",
    "texts = []\n",
    "for c in class_list:\n",
    "    c_dir = '%s/%s' % (text_dir, c)\n",
    "    for file in os.listdir( c_dir ):\n",
    "        if file == 'LICENSE.txt': continue\n",
    "        with open( '%s/%s' % ( c_dir, file ) ) as f:\n",
    "            for _ in range( skipline ): next(f)\n",
    "            text = f.read()\n",
    "            texts.append( text )\n",
    "            y.append( c )\n",
    "y = np.array(y)\n",
    "doc_num = len( texts )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "\n",
    "token_type = ('nva',['名詞', '動詞', '形容詞'])\n",
    "    \n",
    "target_pkl = '%s/livedoor_sentences_%s.pkl' % (corpus_dir, token_type[0])\n",
    "if os.path.exists( target_pkl ):\n",
    "    sentences = load_from_pkl( target_pkl )\n",
    "else:\n",
    "    if token_type[0] == 'all':\n",
    "        sentences = [ get_tokens(text) for text in texts ]\n",
    "    else:\n",
    "        sentences = [ get_tokens_pos(text, token_type[1]) for text in texts ]\n",
    "    save_as_pkl( sentences, target_pkl )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn fastText\n",
    "\n",
    "min_word_count = 10 # Minimum word count\n",
    "context = 10 # Context window size\n",
    "word_embedding = 'fastText'\n",
    "num_features = 100 # fastText embedding dim\n",
    "\n",
    "target_model = '%s/livedoor_%s_%s_%s.model' % ( model_dir, word_embedding, num_features, token_type[0] )\n",
    "\n",
    "if os.path.exists( target_model ):\n",
    "    if word_embedding == 'word2vec':\n",
    "        w2v_model = Word2Vec.load( target_model )\n",
    "    if word_embedding == 'fastText':\n",
    "        w2v_model = FastText.load( target_model )\n",
    "\n",
    "else:\n",
    "    print( word_embedding, num_features, token_type[0] )\n",
    "    if word_embedding == 'word2vec':\n",
    "        w2v_model = Word2Vec(size=num_features, sg=1, workers=7, window=context, min_count=min_word_count)\n",
    "    if word_embedding == 'fastText':\n",
    "        w2v_model = FastText(size=num_features, sg=1, workers=7, window=context, min_count=min_word_count)\n",
    "\n",
    "    sentences = load_from_pkl( '%s/livedoor_sentences_%s.pkl' % (corpus_dir, token_type[0]) )\n",
    "    w2v_model.build_vocab( sentences )\n",
    "    w2v_model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "    # save model\n",
    "    w2v_model.save( target_model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM\n",
    "\n",
    "num_clusters = 30 # cluster num of GMM clustering\n",
    "gmm =  GaussianMixture(n_components=num_clusters, covariance_type=\"tied\", init_params='kmeans', max_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold - 0\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "dokujo-tsushin       0.80      0.84      0.82       174\n",
      "  it-life-hack       0.87      0.90      0.88       174\n",
      " kaden-channel       0.86      0.88      0.87       173\n",
      "livedoor-homme       0.86      0.62      0.72       103\n",
      "   movie-enter       0.87      0.94      0.91       174\n",
      "        peachy       0.78      0.73      0.75       169\n",
      "          smax       0.96      0.97      0.96       174\n",
      "  sports-watch       0.87      0.94      0.91       180\n",
      "    topic-news       0.86      0.82      0.84       154\n",
      "\n",
      "     micro avg       0.86      0.86      0.86      1475\n",
      "     macro avg       0.86      0.85      0.85      1475\n",
      "  weighted avg       0.86      0.86      0.86      1475\n",
      "\n",
      "fold - 1\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "dokujo-tsushin       0.83      0.83      0.83       174\n",
      "  it-life-hack       0.91      0.87      0.89       174\n",
      " kaden-channel       0.85      0.90      0.87       173\n",
      "livedoor-homme       0.90      0.61      0.73       102\n",
      "   movie-enter       0.86      0.97      0.91       174\n",
      "        peachy       0.78      0.76      0.77       169\n",
      "          smax       0.98      0.98      0.98       174\n",
      "  sports-watch       0.91      0.96      0.93       180\n",
      "    topic-news       0.87      0.90      0.89       154\n",
      "\n",
      "     micro avg       0.88      0.88      0.88      1474\n",
      "     macro avg       0.88      0.86      0.87      1474\n",
      "  weighted avg       0.88      0.88      0.87      1474\n",
      "\n",
      "fold - 2\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "dokujo-tsushin       0.84      0.84      0.84       174\n",
      "  it-life-hack       0.88      0.89      0.88       174\n",
      " kaden-channel       0.92      0.87      0.89       173\n",
      "livedoor-homme       0.81      0.62      0.70       102\n",
      "   movie-enter       0.84      0.91      0.88       174\n",
      "        peachy       0.80      0.76      0.78       168\n",
      "          smax       0.93      0.99      0.96       174\n",
      "  sports-watch       0.86      0.92      0.89       180\n",
      "    topic-news       0.83      0.85      0.84       154\n",
      "\n",
      "     micro avg       0.86      0.86      0.86      1473\n",
      "     macro avg       0.86      0.85      0.85      1473\n",
      "  weighted avg       0.86      0.86      0.86      1473\n",
      "\n",
      "fold - 3\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "dokujo-tsushin       0.79      0.85      0.82       174\n",
      "  it-life-hack       0.88      0.88      0.88       174\n",
      " kaden-channel       0.87      0.90      0.88       173\n",
      "livedoor-homme       0.94      0.59      0.72       102\n",
      "   movie-enter       0.90      0.97      0.94       174\n",
      "        peachy       0.78      0.73      0.76       168\n",
      "          smax       0.94      0.97      0.96       174\n",
      "  sports-watch       0.91      0.96      0.93       180\n",
      "    topic-news       0.88      0.90      0.89       154\n",
      "\n",
      "     micro avg       0.87      0.87      0.87      1473\n",
      "     macro avg       0.88      0.86      0.86      1473\n",
      "  weighted avg       0.88      0.87      0.87      1473\n",
      "\n",
      "fold - 4\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "dokujo-tsushin       0.77      0.79      0.78       174\n",
      "  it-life-hack       0.85      0.90      0.87       174\n",
      " kaden-channel       0.84      0.90      0.87       172\n",
      "livedoor-homme       0.84      0.47      0.60       102\n",
      "   movie-enter       0.87      0.92      0.90       174\n",
      "        peachy       0.71      0.72      0.72       168\n",
      "          smax       0.94      0.98      0.96       174\n",
      "  sports-watch       0.93      0.97      0.95       180\n",
      "    topic-news       0.92      0.88      0.90       154\n",
      "\n",
      "     micro avg       0.85      0.85      0.85      1472\n",
      "     macro avg       0.85      0.84      0.84      1472\n",
      "  weighted avg       0.85      0.85      0.85      1472\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# learn (5-fold cv)\n",
    "\n",
    "acc_score_cv = []\n",
    "kf = StratifiedKFold(n_splits=5, random_state=777, shuffle=True)\n",
    "for idx, (train, val) in enumerate( kf.split(sentences, y) ):\n",
    "    \n",
    "    print( \"fold -\", idx )\n",
    "    sentences_train = [ sentences[i] for i in train ]\n",
    "    sentences_val = [ sentences[i] for i in val ]\n",
    "\n",
    "    # train scdv\n",
    "    scdv_model = SCDV(w2v_model=w2v_model, sc_model=gmm, sparse_percentage = 0.04 )\n",
    "    scdv_model.precompute_word_topic_vector(sentences_train)\n",
    "    X_train = scdv_model.train(sentences_train)\n",
    "    \n",
    "    # learn model\n",
    "    clf = lgb.LGBMClassifier(n_estimators=10, objective=\"multiclass\")\n",
    "    clf.fit(X_train, y[train])\n",
    "    \n",
    "    # test scdv\n",
    "    X_val = scdv_model.infer_vector(sentences_val)\n",
    "\n",
    "    # predict validation and score\n",
    "    preds = clf.predict(X_val)\n",
    "    acc_cv = accuracy_score( preds, y[val] )\n",
    "    acc_score_cv.append( acc_cv )\n",
    "    \n",
    "    # classification report\n",
    "    print( classification_report(y[val], preds) )\n",
    "\n",
    "# cv score\n",
    "mean = sum( acc_score_cv ) / len( acc_score_cv )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8654796218792262"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
